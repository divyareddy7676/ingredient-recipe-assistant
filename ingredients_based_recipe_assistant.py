{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# # üß∫ Ingredients Based Recipe Assistant \n# ### Capstone Project - Gen AI Intensive Course 2025Q1\n# \n# This assistant helps users come up with creative and nutritious meals based on ingredients they have or crave. It uses a lightweight **LangGraph agent** powered by **Gemini** to take user input (or use preloaded examples), suggest grounded recipe ideas, and even evaluate which recipe is easiest and most nutritious.\n# \n# You can type your own meal prompt (e.g., *\"I want something spicy with oats and kale in Mexican style\"*), or skip input to see a demo run.\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## ‚úÖ Gen AI Capabilities Demonstrated\n# \n# - **Agents** (LangGraph handles structured interaction flow)\n# - **Function Calling** (via structured node-to-node execution)\n# - **Grounding** (Gemini generates grounded recipe responses)\n# - **Gen AI Evaluation** (Gemini evaluates the recipes)\n# - **Structured Output** (Markdown-formatted responses)\n# - **RAG** (Vector search of past recipes enhances suggestions)\n# - **Embeddings** (Google Generative AI Embeddings)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## üì¶ Setup and Installation\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:42.175198Z\",\"iopub.execute_input\":\"2025-04-21T04:52:42.175580Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.340148Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:42.175556Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.338875Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n!pip install -U -q langgraph langchain langchain-community langchain-google-genai google-genai faiss-cpu google-ai-generativelanguage==0.6.15\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## üîê API Key Setup\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.341866Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.342223Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.419770Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.342197Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.418757Z\"}}\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## ü§ñ LangGraph Agent Setup\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.420529Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.420775Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.431953Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.420757Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.431013Z\"}}\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing_extensions import TypedDict\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\nfrom IPython.display import Markdown, display\n\nclass RecipeState(TypedDict):\n    messages: Annotated[list, add_messages]\n    ingredients: list[str]\n    finished: bool\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.434521Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.434785Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.452488Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.434762Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.451609Z\"}}\ndef chatbot_node(state: RecipeState) -> RecipeState:\n    # Use static prompt directly\n    user_input = \"I want a Mexican dinner with rice and beef\"\n\n    prompt = f\"\"\"\nYou are a smart cooking assistant.\n\nThe user gave this request:\n\\\"{user_input}\\\"\n\nPlease do the following:\n1. Extract:\n   - Ingredients\n   - Preferred cuisine\n   - Meal type\n   - Flavor/style (if mentioned)\n\n2. Based on that, suggest one recipe.\n\nRespond using this format:\n\n### üìù Recipe Summary\n- **Recipe Name**:\n- **Cuisine**:\n- **Flavor/Style**:\n- **Meal Type**:\n- **Main Ingredients**:\n\n### üßæ Ingredients List\n- item 1\n- item 2\n\n### üî™ Instructions\n1. Step one...\n2. Step two...\n\"\"\"\n\n    # Step 1: Generate recipe\n    response = llm.invoke([(\"user\", prompt)])\n    recipe_text = response.content\n    display(Markdown(\"### üçΩÔ∏è Gemini Recipe Suggestion\\n\" + recipe_text))\n\n    # Step 2: Evaluate it using the actual response\n    eval_prompt = f\"\"\"\nPlease evaluate the following recipe based on:\n- How easy it is to prepare\n- How nutritious it is\n\nRecipe:\n{recipe_text}\n\"\"\"\n    eval_response = llm.invoke([(\"user\", eval_prompt)])\n    display(Markdown(\"### üß† Gemini Evaluation\\n\" + eval_response.content))\n\n    print(\"‚úÖ Gemini has successfully suggested and evaluated your recipe!\")\n\n    return {\n        \"messages\": [],\n        \"ingredients\": [],\n        \"finished\": True\n    }\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ### üë§ User Input with Optional Fallback\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.453450Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.453772Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.472966Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.453749Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.471954Z\"}}\ndef user_input_node(state: RecipeState) -> RecipeState:\n    # Static example message\n    user_input = \"I want a Mexican dinner with rice and beef\"\n\n    return {\n        \"ingredients\": [],\n        \"messages\": [(\"user\", user_input)],\n        \"finished\": True  # ‚úÖ Exit after one full pass\n    }\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.473942Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.474211Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.494964Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.474190Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.494088Z\"}}\ndef router(state: RecipeState):\n    return END if state.get(\"finished\") else \"chatbot\"\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## üîç RAG: Embedding + Vector Search + Gemini\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.496001Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.496379Z\",\"iopub.status.idle\":\"2025-04-21T04:52:47.876096Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.496332Z\",\"shell.execute_reply\":\"2025-04-21T04:52:47.875112Z\"}}\nfrom langchain.vectorstores import FAISS\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain.schema import Document\n\nembedding = GoogleGenerativeAIEmbeddings(\n    model=\"models/embedding-001\",\n    google_api_key=GOOGLE_API_KEY\n)\n\npast_recipes = [\n    Document(page_content=\"Savory oatmeal with kale, mushrooms, and carrots.\"),\n    Document(page_content=\"Black bean and chocolate oatmeal.\"),\n    Document(page_content=\"Carrot and kale fritters with bean dip.\"),\n]\n\nvector_db = FAISS.from_documents(past_recipes, embedding)\n\ndef retrieve_similar_and_respond(query: str):\n    results = vector_db.similarity_search(query, k=2)\n    context = \"\\n\".join(doc.page_content for doc in results)\n\n    rag_prompt = f\"\"\"User query: {query}\n\nHere are similar recipes:\n{context}\n\nUsing these as context, suggest a new recipe.\"\"\"\n\n    response = llm.invoke([(\"user\", rag_prompt)])\n    display(Markdown(\"### üß† RAG-enhanced Gemini Response\\n\" + response.content))\n     # üîç Evaluate the generated recipe\n    eval_prompt = f\"\"\"\nPlease evaluate the following recipe based on:\n- Ease of preparation\n- Nutrition\n\nRecipe:\n{response.content}\n\"\"\"\n    eval_response = llm.invoke([(\"user\", eval_prompt)])\n    display(Markdown(\"### üß† Gemini Evaluation (RAG Recipe)\\n\" + eval_response.content))\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:47.877115Z\",\"iopub.execute_input\":\"2025-04-21T04:52:47.877425Z\",\"iopub.status.idle\":\"2025-04-21T04:52:55.972557Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:47.877398Z\",\"shell.execute_reply\":\"2025-04-21T04:52:55.971698Z\"}}\n# üß™ Example RAG usage\nretrieve_similar_and_respond(\"What can I make with mushrooms, oats, and kale?\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## ‚ñ∂Ô∏è Run the LangGraph Agent\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-21T04:52:55.973683Z\",\"iopub.execute_input\":\"2025-04-21T04:52:55.973992Z\",\"iopub.status.idle\":\"2025-04-21T04:52:55.985461Z\",\"shell.execute_reply.started\":\"2025-04-21T04:52:55.973964Z\",\"shell.execute_reply\":\"2025-04-21T04:52:55.984521Z\"}}\ngraph = StateGraph(RecipeState)\ngraph.add_node(\"chatbot\", chatbot_node)\ngraph.add_node(\"user_input\", user_input_node)\ngraph.set_entry_point(\"user_input\")\ngraph.add_conditional_edges(\"user_input\", router)\ngraph.add_edge(\"chatbot\", \"user_input\")\nrecipe_agent = graph.compile()\nrecipe_agent_invoke_msg = recipe_agent.invoke({\"ingredients\": [], \"messages\": [], \"finished\": False})\nprint(\"‚úÖ Recipe suggestion and evaluation complete.\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## ‚úÖ Session Complete\n# \n# Gemini successfully generated and evaluated a recipe based on your prompt.\n# \n# Feel free to update the static input in `chatbot_node()` and re-run the notebook!","metadata":{"_uuid":"84ff526d-4f52-4a0b-b2db-f3aceb67b1e1","_cell_guid":"f1a0d9f5-3f7d-48fc-a129-07c04aac8f85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}